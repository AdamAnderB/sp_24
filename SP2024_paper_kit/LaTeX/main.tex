\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2021}
\usepackage{subfiles}
\usepackage{datatool}
\usepackage{fmtcount}
\usepackage{xstring}
\usepackage{substr} % Include this in your preamble
\usepackage{tipa}
\DTLloaddb{cronbach}{scripts/data_output/cronbach_alpha.csv}
\DTLloaddb{modelsouts}{scripts/data_output/dynamic_models.csv}
\DTLloaddb{partrem}{scripts/data_output/participant_removal.csv}
\DTLloaddb{taskrem}{scripts/data_output/taskremoval.csv}

\newcommand{\livedata}[2]{%
    \DTLfetch{#1}{Statistic}{#2}{Value}%
}

\title{Measuring music and prosody: accounting for variation in non-native speech discrimination with L1, L2, music skills, and working memory}
%Paper submission must be anonymous. Only fill in author information for the final PDF.
\name{Author Anonymous$^1$, 
Co-author Anonymous$^1$,
Co-author Anonymous$^1$,
Co-author Anonymous$^1$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$Author Affiliation}
\email{author@university.edu, 
coauthor@company.com}

\begin{document}

\maketitle
% 
\begin{abstract}
The dynamics of non-native speech perception remain poorly understood, especially in accounting for specialized skills/training. One such skill, musical ability, has been shown to positively impact sensitivity to speech sounds, yet how musical ability is operationalized and measured varies from study to study. Individuals’ musical abilities vary in exposure-duration, skill type (e.g., voice, percussion), and skill-level. Here, we take an individual differences (n=38) approach to explore sensitivity in non-native speech discrimination of prosodic contrasts. We measure language background, general cognitive measures, and three measures of musical ability: auditory-motor temporal integration \cite{Kachlicka_Saito_Tierney_2019}, auditory discrimination \cite[MET;]{Wallentin_Nielsen_Friis-Olivarius_Vuust_Vuust_2010}), and musical sophistication \cite[Gold-MSI;]{Müllensiefen_Gingras_Musil_Stewart_2014}. We measured prosodic sensitivity using three AX discrimination tasks and signal detection measures (d'/c): Mandarin tone (primarily cued by pitch), Italian and Japanese (non-)geminates (primarily cued by duration). Results suggest music background, discrimination, and auditory-motor temporal integration capture related –yet divergent– aspects of music experience. Additionally, music sub-skills (e.g., pitch perception) have unequal contributions to non-native speech sensitivity across languages' respective linguistic cues (e.g., tone). Findings support models of non-native speech perception, which consider cognitive factors and auditory experience outside of language experience.

\end{abstract}
\noindent\textbf{Index Terms}:  Individual differences, Music, Non-native speech perception, Measuring prosody


\section{Introduction}
awesome text

This is gs alpha
\livedata{cronbach}{gs}

this is item removal for italian task:
\livedata{taskrem}{italian.remove}

this is participants removal for language task:
\livedata{taskrem}{particip_remove_lang.remove}

This is the mandarin model out:
\livedata{modelsouts}{Mandarin.(Intercept).estimate}

This is how many participants we removed lang removal:
\livedata{taskrem}{part_remove_lang.before}

This is how many participants we kept:
\livedata{partrem}{kept_participants}

This is how many participants we started with for the cmu data:
\livedata{partrem}{data_exp_141883-v12 .before}

This is how many participants we kept for the cmu data:
\livedata{partrem}{data_exp_141883-v12 .after}
This is gs alpha
\livedata{cronbach}{gs}

this is item removal for italian task:
\livedata{taskrem}{italian.remove}

this is participants removal for language task:
\livedata{taskrem}{particip_remove_lang.remove}

This is the mandarin model out:
\livedata{modelsouts}{Mandarin.(Intercept).estimate}

This is how many participants we removed lang removal:
\livedata{taskrem}{part_remove_lang.before}

This is how many participants we kept:
\livedata{partrem}{kept_participants}

This is how many participants we started with for the cmu data:
\livedata{partrem}{data_exp_141883-v12 .before}

This is how many participants we kept for the cmu data:
\livedata{partrem}{data_exp_141883-v12 .after}

This is how many participants we started with for the cmu data:
\livedata{partrem}{data_exp_141883-v12 .before}

This is how many participants we kept for the cmu data:
\livedata{partrem}{data_exp_141883-v12 .after}

\section{Outline}
\subfile{outline.tex}

\section{methods}

\subsubsection{participants}
 The recruitment of \livedata{partrem}{starting_participants} participants was managed through Prolific \cite{Palan_2018} (n=\livedata{partrem}{data_exp_142778-v2.before})  and in-person (n=\livedata{partrem}{data_exp_141883-v12.before}) recruitment. Additionally, 22 potentional participants were rejected from participation due to failing initial requirements (i.e., eight removed for failed headphone-check \cite{milne_2021} and 14 removed for eye-tracking calibration failure, which is not reported here). To ensure data quality and maximize retained participants, three median absolute deviations (MAD) from median score was calculated as the standard for removal for each task \cite{Leys_2013}. Of the \livedata{partrem}{starting_participants} participants who remained, \livedata{partrem}{removed_participants} were removed for low accuracy scores. Of these, \livedata{taskrem}{particip_remove_lang.remove} were removed for being below MAD range in the language tasks and \livedata{taskrem}{rhythm_part.remove} removed for low performance in auditory-motor integration task.  After removal, \livedata{partrem}{kept_participants} participants (age: $\mu$ = \livedata{partrem}{mean_age}, $\sigma$ = \livedata{partrem}{sd_age}) data were retained for analysis. 

\subsubsection{procedure}
All of the participants took part in a multi-stage battery of language, music, and domain general tasks. Participants first took part in a two-part headphone check, an aural attention check and a dichotic-pitch task \cite{milne_2021}, followed by an 8 trial adaptive staircase digit-span task. After completing the head-phone check and digit-span tasks,
all participants took part in either music or language segments, followed by the complementary segment (i.e., music $\rightarrow$ language, language $\rightarrow$ music). For all language and music tasks, stimuli and experimental design were adapted and built in Gorilla \cite{gorilla_Anwyl-Irvine_2019} for the current study from previous in-person studies (e.g., Mandarin \cite{Wiener_Bradley_2020}) that either made materials openly available on OSF \cite{OSF} or were provided by the original authors. 

In the language segment, participants took part in three speeded AX-discrimination tasks for Italian, Japanese, and Mandarin (trials automatically proceeded after 1000 ms if not responded to)\cite{Hayes‐Harb_Barrios_2021}. All Language stimuli were sampled at 44.1 kHz and recorded in sound attenuated booths (Mandarin) or studio (Japanese and Italian). The Italian and Japanese AX tasks, adapted from \cite{Tsukada_Cox_Hajek_Hirata_2017}, consisted of stop geminate contrasts: Italian - /\textipa{p t k b d g dZ}/, Japanese - /\textipa{t k tS}/. For the Italian AX task, stimuli consisted of 27 pairs of geminate contrasts (e.g., without geminate /\textipa{Eko}/ \textit{echo}, with geminate /\textipa{Ek\textlengthmark o}/ \textit{here}), which were made up of approximately half real and non-real words, spoken by three native speakers. For the Japanese AX task, stimuli consisted of 33 pairs of geminate contrasts (e.g., without geminate: /kate/ \textit{win}, with geminate: /katːe/ \textit{buying}), with approximately half of the pairs matching (e.g., without geminate high-low pitch accent /kate/, with geminate low-high pitch accent /katːe/) and mismatching (without geminate low-high pitch accent /heta/ \textit{unskilled}, without geminate low-high pitch accent /hetːa/ \textit{decreased}) in pitch-accent. The Mandarin task, adpated from \cite{Wiener_Bradley_2020}, consisted of 8 stimuli based on the Mandarin syllable yu with four tones (yu1, yu2, yu3, yu4) recorded by male and female native speakers and were normalized for F0 manipulation using \cite{Boersma_Weenink}'s method. Each tonal pairing co-occured equal amounts with each tone occurring with itself three times to equalize match-mismatch answers across the task. 

The music segment has two basic tasks: auditory-motor temporal integration \cite{Kachlicka_Saito_Tierney_2019} and auditory discrimination \cite[MET;]{Wallentin_Nielsen_Friis-Olivarius_Vuust_Vuust_2010}), both of which have melody and rhythm sections. In the auditory-motor integration tasks, participants hear a rhythm (auditory-motor rhythm) or melody(auditory-motor rhythm) three times, memorizes it, and reproduces either the melodic and rhythmic pattern. For example, in the auditory-motor rhythm task each trial plays a 13 beat rhythm three times. The participant then needs needs to repeat the exact beat using the space-bar. Timing is captured on each space-bar press. Similarly, for the auditory-motor melody task a series of 7 notes are played, the participants then needs to repeat these with a series of on-screen buttons on the screen that correspond to pitches. After completion of both auditory-motor temproal integration tasks, the participant then does a auditory AX discrimination task, where two auditory music samples (melody or rhythm) are played then participants must determine if the samples were identical (melodies or drum beats) through a button press on the screen. 

After completing the music and language segments, participants then fill in both a music sophisitication survey \cite[Gold-MSI;]{Müllensiefen_Gingras_Musil_Stewart_2014} and a Language Experience and Proficiency Questionnaire (LEAP-Q) \cite{Marian_Blumenfeld_Kaushanskaya_2007}. The Gold-MSI is a self-reported survey, which aims to capture individual differences through musical sophistication in 5 areas: Active Musical Engagement e.g., time and money resources spent on music; Self-reported Perceptual Abilities, e.g., musical listening skills
Musical Training; e.g. formal musical training
Self-reported Singing Abilities, e.g. one’s own singing; Emotional Engagement with Music, e.g. ability to talk about emotions in music). Finally, the participant fills out the LEAP-Q with self-reported data on their language proficiency and experience of bilingual and multilingual practices. 

\subsection{Data Analysis}
\subsubsection{Data Wrangling and Validation}


Reaction time based data removal was calculated by overall trial log transformed reaction time on both the musical ear tasks (rhythm and melody). MAD was used as the removal standard, reaction time trial data outside three deviations was removed (proportion of trials removed from musical ear tasks: \livedata{taskrem}{RTmet.remove} of \livedata{taskrem}{RTmet.before} trials). Language tasks were transformed using the squareroot transformation due to larger skew that is likely due to the speeded nature of the AX tasks. No trials data was removed from language tasks due to the automatic progression of trials at 1000ms.

Data removal based on accuracy was then calculated by item for both Musical Ear tasks and for each language task. The standard of three MADs was applied to each task's accuracy. Removal was conservative for each task, retained items for each task: Auditory-motor melody items \livedata{taskrem}{melody_item.after}/\livedata{taskrem}{melody_item.before}, Auditory-motor rhythm items \livedata{taskrem}{rhythm_item.after}/\livedata{taskrem}{rhythm_item.before}, Mandarin items \livedata{taskrem}{Mandarin.after}/\livedata{taskrem}{Mandarin.before}, Italian items \livedata{taskrem}{italian.after}/\livedata{taskrem}{italian.before}, Japanese items \livedata{taskrem}{japanese.after}/\livedata{taskrem}{japanese.before}. Additionally in the auditory-motor rhythm task, the first beat of every trial (rhythmic phrase) was removed. This was done because the first beat acted as the trigger for beginning the capture of rhythm. As each possible beat is 200 ms apart, the second beat of each rhythmic phrase was then centered by subtracting 100 ms, as seen in fig

After data removal, Cronbach's alpha was calculated for each of the musical tasks and questionnairre as tested in original studies (i.e., Goldsmiths 5 area scores, Musical Ear melody and rhythm items, auditory-motor melody, auditory-motor rhythm) to test internal reliability of items in each task. Results as compared to original studiest are presented in Table~\ref{tab:comparison}. 

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Task} & \textbf{Reported Value} & \textbf{Our Value} \\
\hline
GoldSmiths \cite{Müllensiefen_Gingras_Musil_Stewart_2014} & Range: 0.79 - 0.93 & 0.89 \\
Musical Ear tasks \cite{Wallentin_Nielsen_Friis-Olivarius_Vuust_Vuust_2010}& 0.87 & 0.79 \\
Auditory-motor melody \cite{Kachlicka_Saito_Tierney_2019}& Unreported & 0.93 \\
Auditory-motor rhythm\cite{Kachlicka_Saito_Tierney_2019}& Unreported & 0.91 \\
\hline
\end{tabular}
\caption{Comparison of Reported and Our Cronbach's alpha values}
\label{tab:comparison}
\end{table}

For both Musical Ear tasks and each Language task,  D-prime (d′) was calculated as a measure of sensitivity and to account for AX-discrimination response bias \cite{Macmillan_Creelman_2004}.

\subsubsection{Statistical analysis}



\subsection{Figures}

stuff here Figure~\ref{fig:raw_sorted_data}.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{SP_24_visuals/by_gs.pdf}
  \caption{Raw data}
  \label{fig:raw_sorted_data}
\end{figure*}
stuff here Figure~\ref{fig:raw_data}.


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig:Correct_and_Incorrect_distrubutions_by_beat_across_trial.pdf}
  \caption{Raw data}
  \label{fig:beat_data}
\end{figure}

\subsection{Results}

all of our fancy results

~\ref{fig:model}.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{SP_24_visuals/Japanese,Italian,_Mandarin_max_models_structure:_parsimonious_effects.pdf}
  \caption{model output}
  \label{fig:model}
\end{figure}

\section{Discussion}

\section{Conclusions}

Complex measurement stuff. But, still many things to discover

\section{Acknowledgements}

We would like to thank XXXX and XXXX for funding this project. \\

\bibliographystyle{IEEEtran}

\bibliography{my_references.bib}

\end{document}
